At Netflix  our mission is to entertain the world. With 200+ million paid members in over 190 countries on millions of devices; enjoying TV series  documentaries  and feature films across a wide variety of genres and languages - Netflix is reinventing entertainment from end to end. We are revolutionizing how shows and movies are produced  pushing technological boundaries to efficiently deliver streaming video at a massive scale over the internet  and continuously improving the end-to-end user experience with Netflix across their member journey.We pride ourselves on using data to inform our decision-making as we work towards our mission. This requires curating data across various domains such as Growth  Finance  Product  Content  and Studio. All of this data collection and curation is made possible thanks to the amazing Data Engineers of Netflix who bring this data to life.Data Engineering at Netflix is a role that requires building systems to process data efficiently and modeling the data to power analytics. These solutions can range from batch data pipelines that bring to life business metrics to real-time processing services that integrate with our core product features. In addition  we require our Data Engineers to have a rich understanding of large distributed systems on which our data solutions rely. Candidates should have knowledge across several of these skill sets and usually need to be deep in at least one. As a Data Engineer  you also need to have strong communication skills since you will need to collaborate with business  engineering  and data science teams to enable a culture of learning. Learn more about the work of data engineers at Netflix.Location of work: We are considering candidates who are willing to relocate to Los Gatos  California  as well as fully-remote candidates (remote in the US with occasional visits to Los Gatos) depending on the team your skills are most aligned with.Who are you?You strive to write elegant code  and you're comfortable with picking up new technologies independentlyYou are proficient in at least one major programming language (e.g. Java  Scala  Python) and comfortable working with SQLYou enjoy helping teams push the boundaries of analytical insights  creating new product features using data  and powering machine learning modelsYou have a strong background in at least one of the following: distributed data processing or software engineering of data services  or data modelingYou are familiar with big data technologies like Spark or Flink and comfortable working with web-scale datasetsYou have an eye for detail  good data intuition  and a passion for data qualityYou appreciate the importance of great documentation and data debugging skillsYou relate to and embody many of the aspects of the Netflix Culture. You love working independently while also collaborating and giving/receiving candid feedbackYou are comfortable working in a rapidly changing environment with ambiguous requirements. You are nimble and take intelligent risksOur compensation structure consists solely of an annual salary; we do not have bonuses. You choose each year how much of your compensation you want in salary versus stock options. To determine your personal top of market compensation  we rely on market indicators and consider your specific job family  background  skills  and experience to determine your compensation in the market range. The range for is $170 000 - $720 000.Netflix provides comprehensive benefits including Health Plans  Mental Health support  a 401(k) Retirement Plan with employer match  Stock Option Program  Disability Programs  Health Savings and Flexible Spending Accounts  Family-forming benefits  and Life and Serious Injury Benefits. We also offer paid leave of absence programs. Full-time hourly employees accrue 35 days annually for paid time off to be used for vacation  holidays  and sick paid time off. Full-time salaried employees are immediately entitled to flexible time off. See more detail about our Benefits here.Netflix is a unique culture and environment. Learn more here.We are an equal-opportunity employer and celebrate diversity  recognizing that diversity of thought and background builds stronger teams. We approach diversity and inclusion seriously and thoughtfully. We do not discriminate on the basis of race  religion  color  ancestry  national origin  caste  sex  sexual orientation  gender  gender identity or expression  age  disability  medical condition  pregnancy  genetic makeup  marital status  or military service.,Not Applicable,Full-time
This is a remote position. Junior Data Engineer (1 year experience  remote)Be part of our future! This job posting builds our talent pool for potential future openings. We'll compare your skills and experience against both current and future needs. If there's a match  we'll contact you directly. No guarantee of immediate placement  and we only consider applications from US/Canada residents during the application process. Hiring Type: Full-Time Base Salary: $57K-$67K Per Annum.Position SummaryJoin the fast-paced  innovative  and collaborative environment focused on providing an AIOps platform that enhances the intelligence of the CVS Health infrastructure. Work closely with subject matter experts and colleagues to build and scale out machine learning and AI solutions that will detect  predict  and recommend solutions to correct issues before system impact and enhance the efficiency  reliability  and performance of CVS Health’s IT operations.Key Responsibilities include:Data pipeline development: Designed  implemented  and managed data pipelines for extracting  transforming  and loading data from various sources into data lakes for processing  analytics  and correlation. Data modeling: Create and maintain data models ensuring data quality  scalability  and efficiency Develop and automate processes to clean  transform  and prepare data for analytics  ensuring data accuracy and consistency Data Integration: Integrate data from disparate sources  both structured and unstructured to provide a unified view of key infrastructure platform and application data Utilize big data technologies such as Kafka to process and analyze large volumes of data efficiently Implement data security measures to protect sensitive information and ensure compliance with data and privacy regulation Create/maintain documentation for data processes  data flows  and system configurations Performance Optimization- Monitor and optimize data pipelines and systems for performance  scalability and cost-effectiveness Characteristics of this role:Team Player: Willing to teach  share knowledge  and work with others to make the team successful. Communication: Exceptional verbal  written  organizational  presentation  and communication skills. Creativity: Ability to take written and verbal requirements and come up with other innovative ideas. Attention to detail: Systematically and accurately research future solutions and current problems. Strong work ethic: The innate drive to do work extremely well. Passion: A drive to deliver better products and services than expected to customers. Required Qualifications2+ years of programming experience in languages such as Python  Java  SQL 2+ years of experience with ETL tools and database management (relational  non-relational) 2+ years of experience in data modeling techniques and tools to design efficient scalable data structures Skills in data quality assessment  data cleansing  and data validation Preferred QualificationsKnowledge of big data technologies and cloud platforms Experience with technologies like PySpark  Databricks  and Azure Synapse. EducationBachelor’s degree in Computer Science  Information Technology  or related field  or equivalent working experience,Mid-Senior level,Full-time
This is a remote position.Junior Data Engineer - Remote Job  1+ Year ExperienceAnnual Income: $57K - $77KA valid work permit is necessary in the USAbout us: Patterned Learning is a platform that aims to help developers code faster and more efficiently. It offers features such as collaborative coding  real-time multiplayer editing  and the ability to build  test  and deploy directly from the browser. The platform also provides tightly integrated code generation  editing  and output capabilities.Are you a passionate and detail-oriented individual with a knack for problem-solving? Do you thrive in a fast-paced  collaborative environment? If so  then this Junior Data Engineer role at CVS Health could be your perfect match!In this exciting role  you'll play a key role in building and scaling our cutting-edge AIOps platform. You'll work alongside a talented team to develop machine learning and AI solutions that will revolutionize CVS Health's IT operations.Here's what you'll do:Design  implement  and manage data pipelines to extract  transform  and load data for analysis and insights. Develop and automate data cleaning  transformation  and preparation processes to ensure data quality and consistency. Integrate data from various sources to create a unified view of our IT infrastructure and applications. Leverage big data technologies like Kafka to handle large data volumes efficiently. Implement data security measures to safeguard sensitive information. Create and maintain documentation for data processes  data flows  and system configurations. Continuously monitor and optimize data pipelines and systems for performance  scalability  and cost-effectiveness. To be successful  you'll need:2+ years of programming experience in Python  Java  and SQL. 2+ years of experience with ETL tools and database management (relational and non-relational). 2+ years of experience with data modeling techniques and tools for designing efficient data structures. Strong skills in data quality assessment  cleansing  and validation. Excellent communication  collaboration  and problem-solving skills. A meticulous attention to detail and a strong work ethic. Bonus points if you have:Knowledge of big data technologies and cloud platforms (e.g.  Azure Synapse). Experience with PySpark and Data-bricks. We offer:The opportunity to work on a cutting-edge AIOps platform that's transforming healthcare. A collaborative and supportive work environment where you can learn and grow. A chance to make a real impact on the success of CVS Health. Please note: While this role is fast-paced  we value work-life balance and offer a comprehensive benefits package.Why Patterned Learning LLC?Patterned Learning can provide intelligent suggestions  automate repetitive tasks  and assist developers in writing code more effectively. This can help reduce coding errors  improve productivity  and accelerate the development process.Pattern recognition is particularly relevant in the context of coding. Neural networks  intense learning models  are commonly employed for pattern detection and classification tasks. These models simulate human decision-making and can identify patterns in data  making them well-suited for tasks like code analysis and generation.,Mid-Senior level,Full-time
Company DescriptionAt Xplor we help businesses succeed by giving them the software  payments  and commerce-accelerating technologies they need to thrive.We know our clients and partners because we once were our clients. We understand their industry  customers  and unique goals so that we can help them overcome obstacles and leave a lasting legacy.Job DescriptionWrite and optimize SQL queries for data retrieval  manipulation  and analysis.Develop stored procedures  functions  and triggers for efficient data processing and management.Utilize advanced SQL techniques to perform data transformations and aggregations.Manage and maintain relational databases including SQL Server  MySQL  PostgreSQL  and Snowflake.Design and implement database schemas  tables  indexes  and views to support application requirements.Design  develop  and implement ETL (Extract  Transform  Load) processes to integrate data from various sources into data warehouses or data lakes.Ensure the reliability  scalability  and efficiency of ETL pipelines for large-scale data processing.Identify and resolve data quality issues through data profiling  cleansing  and normalization techniques.Design and maintain dimensional data models for data warehouses to support reporting and analytics requirements.Work closely with data architects and analysts to understand data requirements and translate them into effective data models.QualificationsBachelor's degree in Computer Science  Information Technology  or a related field.Hands-on experience with SQL Server  MySQL  PostgreSQL  and Snowflake.Proficiency in writing complex SQL queries and optimizing database performance.Strong understanding of data warehousing concepts and dimensional modeling techniques.Excellent problem-solving skills and attention to detail.Effective communication and collaboration skills in a team environment.Additional InformationLife at XplorYou’ll be part of a global network of talented colleagues who support your success. We look for commonalities and shared passions and give people the tools they need to deliver great work and grow at speed.Some Of The Perks Of Working With Us12 weeks Gender Neutral Paid Parental Leave for both primary and secondary career#GiveBackDays/Commitment to social impact – 3 extra days off to volunteer and give back to your local communityOngoing dedication to Diversity & Inclusion initiatives such as D&I Council  Global Mentorship ProgramAccess to free mental health supportFlexible working arrangementsThe average base salary pay range for this role is between $70 000-$90 000 USDMay be considered for a discretionary bonus More About UsWe're the first global platform combining SaaS with embedded payments  and tools to help businesses grow and succeed. We offer software solutions in fast-growing ‘everyday life’ verticals: Education  Fitness & Wellbeing  Field Services and Personal Services – and a global  cloud-based payments processing platform. Xplor Technologies serves over 106 000 customers that processed over $37 billion in payments  operating across 20 markets in 2023.Good to knowTo be considered for employment  you must be legally authorized to work in the location (country) you're applying for. Xplor does not sponsor visas  either at the time of hire or at any later time.We kindly ask you to apply through our careers portal or external job boards only. Please don't send your application via email.To learn more about us and our products  please visit www.xplortechnologies.com/us/careers.We also invite you to check out our Candidate FAQs for more information about our recruitment process www.xplortechnologies.com/us/recruitment-faqs.Xplor is proud to be an Equal Employment Opportunity employer. We're dedicated to attracting  retaining and developing our people regardless of gender identity  ethnicity  sexual orientation  disability  veteran status and age. Applications are encouraged from all sectors of the community.All Information will be kept confidential according to EEO guidelines.Xplor is committed to the full inclusion of all qualified individuals. In keeping with our commitment  Xplor will take the steps to assure that people with disabilities are provided reasonable accommodations. Accordingly  if reasonable accommodation is required to fully participate in the job application or interview process  to perform the essential functions of the position  and/or to receive all other benefits and privileges of employment  please contact us via talent@xplortechnologies.com.We are a 2024 Circle Back Initiative Employer – we commit to respond to every applicant.,Mid-Senior level,Full-time
Worth AI  a dynamic player in the computer software industry  is seeking a skilled and motivated individual to join their team as a Data Engineer. Worth AI is committed to transforming decision-making using the power of AI while promoting equity  diversity  and inclusion. With core values centered around diversity of thought  teamwork  and adaptability  Worth AI is dedicated to making a positive impact in the business world.As a Data Engineer at Worth AI  you will play a crucial role in designing and implementing data pipelines  data integration solutions  and data infrastructure to support the company's AI initiatives. You will collaborate closely with data scientists  software engineers  and other stakeholders to ensure effective data management and accessibility. Your expertise in data processing  data modeling  and database optimization will be essential in delivering scalable and reliable data solutions.ResponsibilitiesDesign  build  and maintain large-scale data processing systems and architectures that support AI initiativesDevelop and implement data pipelines and ETL processes to ingest  transform  and load data from various sourcesDesign and optimize databases and data storage solutions for high performance and scalabilityCollaborate with cross-functional teams to understand data requirements and ensure data quality and integrityImplement data governance and data security measures to protect sensitive dataMonitor and troubleshoot data infrastructure and pipeline issues in a timely mannerStay up-to-date with the latest trends and technologies in data engineering and recommend improvements to enhance the company's data capabilitiesRequirementsProven experience as a Data Engineer or similar role  preferably in a software or technology-driven company with experience processing several to several hundreds of Gigabytes of data or moreIn-depth knowledge of data modeling  data warehousing  and database design principlesStrong programming skills in Python  SQL  and other relevant languagesExperience with relational and NoSQL databases  such as PostgreSQL  MySQL  MongoDBProficiency in data integration and ETL tools  such as Apache Kafka  Apache Airflow  or InformaticaFamiliarity with big data processing frameworks  such as Hadoop  Spark  or FlinkKnowledge of cloud platforms  such as AWS  Azure  or GCP  and experience with data storage and processing services in the cloudUnderstanding of data governance  data privacy  and data security best practicesStrong problem-solving and troubleshooting skills  with a focus on data quality and system performanceExcellent communication and collaboration skills to work effectively with cross-functional teamsPrior collaborative work with data scientists or machine learning professionals with respect to sourcing  processing and scaling both input and output dataComfortable going through documentation of third-party API's and identifying best procedures for integrating data from API's into broader ETL processes BenefitsHealth Care Plan (Medical  Dental & Vision)Retirement Plan (401k  IRA)Life InsuranceUnlimited Paid Time Off9 paid HolidaysFamily LeaveWork From HomeFree Food & Snacks (for those who are in Orlando  FL)Wellness Resources,Mid-Senior level,Full-time
About KnowBe4KnowBe4  the provider of the world's largest security awareness training and simulated phishing platform  is used by tens of thousands of organizations around the globe. KnowBe4 enables organizations to manage the ongoing problem of social engineering by helping them train employees to make smarter security decisions  every day.Fortune has ranked us as a best place to work for women  for millennials  and in technology for four years in a row! We have been certified as a "Great Place To Work" in 8 countries  plus we've earned numerous other prestigious awards  including Glassdoor's Best Places To Work.Our team values radical transparency  extreme ownership  and continuous professional development in a welcoming workplace that encourages all employees to be themselves. Whether working remotely or in-person  we strive to make every day fun and engaging; from team lunches to trivia competitions to local outings  there is always something exciting happening at KnowBe4.As a Data Engineer at KnowBe4  you'll be pivotal in crafting and refining data pipelines essential for AI project development and internal operations. Your core responsibilities include designing  implementing  and maintaining scalable data infrastructure to facilitate streamlined data processing and analysis.This role presents a unique opportunity to shape KnowBe4's operational landscape with innovative solutions. We seek individuals passionate about harnessing AI and AWS technologies to drive efficiency and elevate user experiences.Moreover  you'll leverage cutting-edge AWS tools like Bedrock  Firehose  Lambda  and EventBridge. By integrating APIs from Salesforce  Netsuite  Zendesk  and other vital business applications  you'll collaborate on tailoring solutions to address our internal users' diverse needs. Join us in revolutionizing internal operations and contributing to a safer digital environment.ResponsibilitiesDevelop  enhance  and fine-tune data pipelines tailored for internal data amalgamation from diverse solutions  optimizing processing and analysis to prime the data for RAG development.Ensure reliability  scalability  and efficiency of data infrastructure components  including databases  data warehouses  and ETL processes  leveraging AWS technologies.Collaborate with data scientists and engineers to support machine learning model development and deployment.Implement best practices for data governance  security  and compliance on AWS.Stay updated on emerging technologies and trends in data engineering and AWS technologies  recommending new tools and techniques.Minimum QualificationsBachelor's or Master's degree in Computer Science  Engineering  or related field.Extensive experience in data engineering  including designing and implementing data pipelines and architectures on AWS.Proficiency in programming languages such as Python or Java and experience with database technologies such as SQL and NoSQL.Strong understanding of CI/CD and Git Ops workflows.Strong analytical and problem-solving skills  for focussing on delivering high-quality solutions.Excellent communication and collaboration skills  with the ability to work effectively in a cross-functional team environment.Proficiency in Python or Java programming languages for data pipeline development and automation.Strong understanding of data engineering concepts and best practices  with hands-on experience in designing and implementing data pipelines.Experience with AWS services such as Amazon S3  Kinesis  Glue  Redshift  DynamoDB  and Athena.Knowledge of database technologies such as SQL and NoSQL  with experience in data modeling and schema design.Familiarity with big data technologies such as Apache Hadoop  Spark  or Kafka.Experience with data visualization tools such as Tableau or Power BI.Familiarity with CI/CD and Git Ops workflows for version control and automated deployment.Strong problem-solving skills and ability to optimize data pipelines for performance  scalability  and reliability.Excellent communication and interpersonal skills  with the ability to effectively collaborate with cross-functional teams.The base pay for this position ranges from $110 000 - $120 000  which will vary depending on how well an applicant's skills and experience align with the job description listed above.We will accept applications until 7/16/24.Our Fantastic BenefitsWe offer company-wide monthly bonuses  employee referral bonuses  401k matching (US)  fully paid medical insurance (US)  open/generous paid time off (length varies by country)  parental leave (length varies by country)  adoption assistance  tuition reimbursement  certification reimbursement  certification completion bonuses  gym benefits  and a relaxed dress code - all in a modern  high-tech  and fun work environment. For more details about our benefits  visit www.knowbe4.com/careers/benefits.Note: An applicant assessment and background check may be part of your hiring procedure.Individuals seeking employment at KnowBe4 are considered without prejudice to race  color  religion  national origin  age  sex  marital status  ancestry  physical or mental disability  veteran status  gender identity  sexual orientation or any other characteristic protected under applicable federal  state  or local law. If you require reasonable accommodation in completing this application  interviewing  completing any pre-employment testing  or otherwise participating in the employee selection process  please visit www.knowbe4.com/careers/request-accommodation.No recruitment agencies  please.,Associate,Full-time
CAI is on the hunt for a dedicated and proficient Data Engineer to enrich our technologyefforts. As a pivotal part of our data team  you will construct and maintain the backbone of our data platform  creating the channels through which data flows seamlessly into our system. This role requires a mix of technical prowess and a keen understanding of business needs  ensuring our data solutions are robust and aligned with our company's vision.Key ResponsibilitiesConstruct  manage  and optimize data pipelines for data ingestion  storage  and provisioning.Work closely with data architects to implement their designs and uphold data standards.Utilize modern data storage  processing tools  and modeling techniques to prepare data for analytical or operational uses.Ensure the integrity and availability of data throughout the data lifecycle.Monitor system performance  identifying bottlenecks and devising solutions to improve data reliability and quality.Collaborate with data scientists and business analysts to support data-driven decisions and machine learning initiatives.Develop and maintain scalable and reliable data infrastructure to meet business requirements.Lead the integration of new data management technologies and software engineering tools into existing structures.QualificationsBachelor’s or Master’s degree in Computer Science  Engineering  or a related technical discipline.At least 3 years of hands-on experience in a data engineering role.Strong command over SQL  Python  and other relevant data manipulation languages.Experience with data modeling  ETL development  and data warehousing solutions  especially with platforms like Snowflake.Demonstrated ability to work with large  complex data sets.Excellent problem-solving skills and attention to detail.Superior communication abilities that let you convey intricate concepts to a non-technical audience with clarity.Proven track record of working in cross-functional teams to deliver stellar project outcomes.Other RequirementsExcellent oral and written communication skills in English/Fluent in EnglishAble to travel domestically and internationally as requiredAble to work in the US without sponsorship now or any time in the futureAbout CAICAI is a 100% employee-owned company established in 1996 that has grown to more than 800 people worldwide. We provide commissioning  qualification  validation  start-up  project management and other consulting services associated with operational readiness to FDA regulated and other mission-critical industries.Meeting a Higher StandardOur approach is simple; we put the client’s interests first  we do not stop until it is right  and we will do whatever it takes to get there.As owners of CAI  we are committed to living our Foundational Principles  both professionally and personally:We act with integrity.We serve each other.We serve society.We work for our future.With employee ownership  one person’s success is everyone’s success; we work diligently to accomplish team goals. We place Team Before Self  demonstrate Respect for Others  and possess a Can-Do Attitude (our core values). That is how we have grown exponentially.BenefitsOur full-time positions offer competitive compensation and benefits which include up to 15% retirement contribution  24 days PTO and 5 sick days per year  health insurance at extremely low cost to employee  financial support for both internal and external professional education as well as 70% long term disability paid for by the company.$122 000 - $155 000 a yearAverage base salary range - not including benefits.We are an equal opportunity employer; we are proud to employ veterans and promote diversity and inclusion in our workplace. Diversity is a strength for our global company. We pledge that CAI will be operated in a way that is fair and equitable to all – our employees  our customers  and the broader society.This job description is not all inclusive and you may be asked to do other duties. CAI will also consider for employment qualified applicants with criminal histories in a manner consistent with the requirements of the Fair Chance Act (FCA) / Fair Chance Ordinance (FCO).,Entry level,Full-time
There are 2 actors on a network  people and machines. Just as usernames and passwords are used by people to access machines  machine identities are used by machines to identify and access each other. Venafi is the inventor of the technology that manages and protects machine identities  the most important security initiative in our Global 5000 customers. We are Warriors!Are you passionate about making a positive impact and protecting the world from cybercriminals? If so  you may be a natural Venafi Warrior!How You’ll Be Protecting The WorldVenafi is looking for a Data Engineer who will report to the Sr. Director of Enterprise Analytics to support their Data as a Service mission. The Data Engineer is responsible for designing  and developing high-performance  resilient  automated data pipelines  and data transformations that feed our cloud-based Enterprise Data Platform. You will work with data analysts to design data integrations that meet organizational needs within our Snowflake data ecosystem. You are comfortable with BI work from the requirements phase through ETL  all the way through the presentation layer of BI. You will also implement and govern Data Management best practices and proactively recommend approaches and solutions. Strong problem-solving skills are needed with the ability to work across multiple projects at a time.The Ideal Venafi Warrior Will Be Armed WithExpert in programming languages like SQL  Python  Scala etc.Working experience with Snowflake - data modelling  ELT using Snowflake SQL  implementing complex stored Procedures and standard DWH and ETL conceptsExpert knowledge of Snowflake concepts like Streams  Tasks  Snowpipes  Zero copy clone  time travel  query profiling  RBAC controls  virtual warehouse sizing and experience using these featuresProven track record in designing complex scalable pipelines using Cloud supported ELT Tools.Take initiative; identify key requirements in dynamic environmentsAbility to communicate effectively and credibly with stakeholders and other team membersExperience implementing Data warehouse  Data lakes in the cloudAbility to create & implement data engineering best practices for the full software development life cycle  including coding standards  code reviews  source control management  documentation  build processes  automated testing  and operations.5+ years of experienceWhile you are busy protecting the world  we’ve got you covered!In addition to fostering a virtual first collaborative environment  Venafi offers a benefits package that is in the top 10%. Venafi pays 90% of the monthly premium for medical insurance and 100% of the monthly premium for dental  vision  life insurance  short and long-term disability  and accident insurance for both team members and their families. We offer an open time off policy and observe 12 holidays each year. We also offer a 401(k) with company matching  company HSA contribution  2x salary employer-paid life insurance  parental leave  pet insurance  fertility  adoption and surrogacy benefits!More About VenafiVenafi is the undisputed leader in Machine Identity Management. Why? Because we created the category and are light years ahead of anyone that would consider competing! Gartner has recognized Venafi as number one in our space and as it turns out  one is NOT the loneliest number!Venafi is the inventor of the technology that secures and protects machine identities. The Venafi platform provides visibility  intelligence  and automation for SSL/TLS  IoT  mobile  cloud native  Kubernetes  and SSH machine identity types. Many of the largest organizations in the world use Venafi.Billions of dollars have been spent protecting usernames and passwords and almost nothing managing machine identities—organizations are just now realizing that managing and protecting machine identities is as important as managing usernames and passwords. The bad guys know this and are using stolen or forged machine identities in their cyberattacks. In fact  Gartner says 50% of network attacks will use machine identities.Come help us protect the world!The anticipated pay range for this position is $98 000 to $115 000. This is a general estimate for informational purposes only. The actual salary offered will be determined based on the candidate’s relevant qualifications  experience  and skills. Upon review of the candidates and based on the objective factors listed above  this position may be filled at a higher or lower tier.,Mid-Senior level,Full-time
Loopio’s Data Enablement Team works together on a mission to transform the RFP response process into a rapid and seamless experience. Our team works across all platform portfolio features and delivers innovative solutions where it matters the most for our customers.We are looking for an experienced Data Engineer to build and support the delivery of data pipelines  used for analytics both internally by Loopio’s teams  embedded within the Loopio platform for our customers. Our perfect candidate has deep technical skills and is comfortable working in an evolving technology infrastructure. This is a unique opportunity to join a growing team of creative and passionate individuals committed to solving real world problems. You will empower engineers  product  and data scientists with tools that enable and streamline data-driven decision making  analysis  and feature development.You will partner closely with ML Engineers  Architects and Data Scientists  Product Managers  and other business stakeholders to help us take Loopio’s data value to the next level.What You'll Be DoingBe responsible for building  evolving and scaling data platforms and ETL pipelines  with an eye towards growth of our business and reliability of our dataPromote data-driven decision making across the organization through data expertiseBuild advanced automation tooling for data orchestration  evaluation  testing  monitoring  administration  and data operationsIntegrate various data sources into our Datalake  including clickstream  relational  and unstructured dataDeveloping and maintaining a feature store for use in analytics & modelingPartner with data scientists to create predictive models to help drive insights and decisions  both in Loopio’s product and internal teams (RevOps  Marketing  CX)Work closely with stakeholders within and across teams to understand the data needs of the business and produce processes that enable a better product and support data-driven decision-makingBuild scalable data pipelines using Databricks  and AWS (Redshift  S3  RDS)  and other cloud technologies Build and support Loopio’s data warehouse (Redshift) and data lake (Databricks deltalake)Orchestrate pipelines using workflow frameworks / tooling What You'll Bring to the Team3+ years experience in a data engineering or a similar roleExperience in a high growth agile software development environmentExperience building and supporting large-scale systems in a production environmentStrong communication  collaboration  and analytical skillsAbility to clearly communicate technical roadmap  challenges  and mitigationDemonstrated ability to work with a high degree of ambiguity  and leadership within a team (mentorship  ownership  innovationStrong understanding of database concepts  modeling  SQL  query optimizationAbility to learn fast and translate data into actionable resultsExperience developing in Python and Pyspark Hands-on experience with the AWS services (RDS  S3  Redshift  Glue  Quicksight  Athena  ECS)Strong understanding of relational databases (RDS  MySQL) and NoSQLExperience with ETL & Data warehousing  building fact & dimensional data modelsExperience with data processing frameworks such as Spark / DatabricksExperience in developing Big Data solutions (migration  storage  processing)Experience with CI/CD tools (Jenkins) and pipeline orchestration tools (Databricks Jobs  Airflow)Experience working with data visualization and BI platforms (Quicksight  Tableau  Sisense  etc)Experience working with Clickstream data (Amplitude  Pendo  etc)Bonus QualificationsExperience with container orchestration (leveraging tools like Docker  ECS  Kubernetes)Experience working with BI PaaS / SaaS solutionsFamiliarity with microservice architectures Experience with Natural Language Processing techniquesExperience using deep learning frameworks such as TensorFlow / PytorchWhere You'll WorkLoopio is a remote-first workplace because we recognize the advantages of working flexibly. We have two Hub Regions  which means that employees live and work within a 300 KM radius of Toronto (within Ontario) or Vancouver (within British Columbia) and work within regular business hours in their timezone. Loopio’s office headquarters are located in Toronto’s vibrant Kensington Market. All Loopers have the option to work from home. Ontario Loopers have the option to work in the Toronto HQ and BC Loopers may work from our co-working office in Gastown Vancouver. It is whatever works best for you!You’ll collaborate with your teams virtually (we’re just a Zoom call away!) and have established core sync hours and focus time during the workday to enable us to work smarter togetherWhy You'll Love Working at LoopioYour manager supports your development by providing ongoing feedback and regular 1-on-1s You have tons of autonomy and responsibility: this role provides an opportunity to try new things and push creative boundariesYou’ll learn more than you thought was possible; our team is obsessed with personal and professional growth (every Looper receives a professional mastery allowance each year)You’ll be set up to work remotely with a MacBook laptop  a monthly phone and internet allowance  and a work-from-home budget to help get your home office all set up! Join us in regular company socials  AMA (Ask-Me-Anything)  and quarterly kick-off to celebrate the big wins and milestones as #oneteam!You’ll be joining a culture that has thoughtfully built out opportunities for connections in a remote first environmentWe have Employee Resource Groups  House Teams (curious? ask us about it!)  virtual yoga  cooking classes and many more moments for us to have fun and learn together! You’ll be a part of an award-winning workplace and one of Canada’s fastest growing companies with ample opportunity to make a big impact here!,Not Applicable,Full-time
Data Engineer Position AvailableTitle: Data EngineerMinimum Qualifications:Thrive in a fast-paced  cutting-edge technology environment.Proven experience with data modeling and developing Enterprise Data Warehouse solutions  including OLTP  OLAP  Dimensions  Facts  and Data modeling.Experience in designing and implementing data pipelines for BI and Machine Learning solutions.Knowledge of data security and privacy best practices.Expertise in Azure Monitor  Load Balancer  Application Gateway  Azure Functions  and Azure Data Factory Integrations.At least 5 years of hands-on experience in Data Management leveraging Microsoft Azure Cloud  including:Data integration and ETL: Azure Data Factory  Power BI Dataflow  SSIS.Database Management: Azure SQL Database  Azure Synapse Analytics  Azure Data Lake Storage  MSSQL  Cosmos DB  Analysis Services.Compute: Azure Functions.Monitoring: Azure Monitor.Container orchestration: Kubernetes.Experience working and communicating cross-functionally in a team environment.Ability to commute to one of our office locations.Preferred Qualifications:Certifications in data engineering AI/ML technologies and Azure  such as Azure AI Engineer  Azure Data Scientist  or Azure Solutions Architect.Responsibilities:Transform structured  semi-structured  and unstructured data into actionable insights.Lead  design  develop  and deliver large-scale data systems  data processing  and data transformation projects.Design and develop scalable  high-performance data pipelines using Azure cloud platform technologies.Build  test  and debug software packages and pipelines required for optimal extraction  loading  and transformation (ELT) of data from various sources.Implement best practices for data management  security  and governance.Document data pipelines  data models  and reports.Summary: We are seeking a Data Engineer to support one of our clients in designing and developing large-scale data systems and high-performance data pipelines using Azure technologies. The ideal candidate thrives in a fast-paced environment and has a minimum of 5 years of experience in data management and integration  with expertise in Azure Data Factory  Azure SQL Database  Azure Synapse Analytics  and Kubernetes. Responsibilities include transforming structured and unstructured data into actionable insights  implementing data security best practices  and documenting data models and pipelines. Preferred candidates will have relevant Azure certifications and experience with data modeling  ETL processes  and BI solutions. Candidates must live within commuting distance to one of our office locations.MUST be a US Citizen. Unfortunately  sponsorship is not available at this moment in time.,Mid-Senior level,Full-time
I solutions. Candidates must live within commuting distance to one of our office locations.MUST be a US Citizen. Unfortunately  sponsorship is not available at this moment in time.,Mid-Senior level,Full-time
 teammate pay regularly to ensure competitive and equitable pay. We also offer a generous suite of benefits. To learn more  visit www.benefityourliferesources.com.,Entry level,Full-time
Genesis10 is seeking a Data Engineer for a contract with our client in Eagan  MN. 100% Remote.Job DescriptionThe Systems Developer is responsible for investigating  identifying  and implementing application system changes and developing new applications. This position is responsible for analyzing and evaluating existing or proposed application systems to process data. This position is also responsiblefor designing  coding  testing  debugging  and documenting programs and collaborating with other team members by providing technical advice and training.Enable and deliver industry leading data-driven reporting and insights using modern data architecture  data science  and machine learning.Key ResponsibilitiesConsult with System Analysts  Technical Leads  and Project Managers to establish specifications and estimations on complex initiatives and requestsWrite detailed description of user needs  system design  program functions  and steps required to modify computer systems; review specifications with System Analysts  Technical Leads  and Business User to ensure complianceWork with System Analysts  Technical Leads  and Business Users to develop and implement acceptance test plan to ensure product meets customer requirements and expectationsAssist technical staff on use of application systems software as needed; assist less experienced staff in user specifications  coding  testing  and debuggingOther duties as assignedInteraction level with Management/Team Members: Daily interaction with team members for tasks such as detailed design reviews  code reviews  and support.Resource will be aligned to foundational initiatives which could be client implementations  enhancements  or compliance related. The focus of this role will be on moving data thru the data environment  running API's  pulling from stored procedures  and pulling from various internal applications.Required Basic QualificationsUnlock health care insights and increase Prime's data IQ to help people feel better and live wellWork with leading cloud-based technologiesGain experience with Big DataDesign  build  and maintain data pipelines end-to-endCollaborate on domain-specific architectureInfluence and establish patterns and best practicesMentor less experienced data engineersParticipate in strategic planning with senior leadershipBachelor's degree in Computer Science or related area of study  or equivalent combination of education and/or relevant work experience; High School Diploma or GED equivalent required5 years of experience with application development or packaged software implementationsRequired Technology:Resource will not need all of the below. They are listed in order of priority.Apache Data FlowGoogle GCPAWSPySparkSnaplogicDataStage or other ETL toolsAlteryxNice to have Technology:GCP Experience with composer (airflow)  google data flow (apache beam)  and operators inside of data flow that interact with Big QuerySoft Skills NeededAbility to work effectively in a cross-functional team and independentlyStrong problem-solving and analysis skillsCompensation:Hourly W2 pay rate $We have access to additional contract  contract-to-hire  and direct hire positions with various rate ranges.If you have the described qualifications and are interested in this exciting opportunity  apply today!Ranked a Top Staffing Firm in the U.S. by Staffing Industry Analysts for six consecutive years  Genesis10 puts thousands of consultants and employees to work across the United States every year in contract  contract-for-hire  and permanent placement roles. With more than 300 active clients  Genesis10 provides access to many of the Fortune 100 firms and a variety of mid-market organizations across the full spectrum of industry verticals.BenefitsFor contract roles  Genesis10 offers the benefits listed below. If this is a perm-placement opportunity  our recruiter can talk you through the unique benefits offered for that particular client. Benefits of Working with Genesis10:Access to hundreds of clients  most who have been working with Genesis10 for 5-20+ years.The opportunity to have a career-home in Genesis10; many of our consultants have been working exclusively with Genesis10 for years.Access to an experienced  caring recruiting team (more than 7 years of experience  on average.)Behavioral Health PlatformMedical  Dental  VisionHealth Savings AccountVoluntary Hospital Indemnity (Critical Illness & Accident)Voluntary Term Life Insurance401KSick Pay (for applicable states/municipalities)Commuter Benefits (Dallas  NYC  SF)Remote opportunities availableFor multiple years running  Genesis10 has been recognized as a Top Staffing Firm in the U.S.  as a Best Company for Work-Life Balance  as a Best Company for Career Growth  for Diversity  and for Leadership  amongst others. To learn more and to view all our available career opportunities  please visit us at our website.Genesis10 is an Equal Opportunity Employer. Candidates will receive consideration without regard to their race  color  religion  sex  sexual orientation  gender identity  national origin  disability  or status as a protected veteran.#DIG10-MN,Entry level,Contract
Data Engineer Position AvailableTitle: Data EngineerMinimum Qualifications:Thrive in a fast-paced  cutting-edge technology environment.Proven experience with data modeling and developing Enterprise Data Warehouse solutions  including OLTP  OLAP  Dimensions  Facts  and Data modeling.Experience in designing and implementing data pipelines for BI and Machine Learning solutions.Knowledge of data security and privacy best practices.Expertise in Azure Monitor  Load Balancer  Application Gateway  Azure Functions  and Azure Data Factory Integrations.At least 5 years of hands-on experience in Data Management leveraging Microsoft Azure Cloud  including:Data integration and ETL: Azure Data Factory  Power BI Dataflow  SSIS.Database Management: Azure SQL Database  Azure Synapse Analytics  Azure Data Lake Storage  MSSQL  Cosmos DB  Analysis Services.Compute: Azure Functions.Monitoring: Azure Monitor.Container orchestration: Kubernetes.Experience working and communicating cross-functionally in a team environment.Ability to commute to one of our office locations.Preferred Qualifications:Certifications in data engineering AI/ML technologies and Azure  such as Azure AI Engineer  Azure Data Scientist  or Azure Solutions Architect.Responsibilities:Transform structured  semi-structured  and unstructured data into actionable insights.Lead  design  develop  and deliver large-scale data systems  data processing  and data transformation projects.Design and develop scalable  high-performance data pipelines using Azure cloud platform technologies.Build  test  and debug software packages and pipelines required for optimal extraction  loading  and transformation (ELT) of data from various sources.Implement best practices for data management  security  and governance.Document data pipelines  data models  and reports.Summary: We are seeking a Data Engineer to support one of our clients in designing and developing large-scale data systems and high-performance data pipelines using Azure technologies. The ideal candidate thrives in a fast-paced environment and has a minimum of 5 years of experience in data management and integration  with expertise in Azure Data Factory  Azure SQL Database  Azure Synapse Analytics  and Kubernetes. Responsibilities include transforming structured and unstructured data into actionable insights  implementing data security best practices  and documenting data models and pipelines. Preferred candidates will have relevant Azure certifications and experience with data modeling  ETL processes  and BI solutions. Candidates must live within commuting distance to one of our office locations.MUST be a US Citizen. Unfortunately  sponsorship is not available at this moment in time.,Mid-Senior level,Full-time
About CalmCalm is on a mission to support everyone on every step of their mental health journey. With the #1 app for sleep  meditation and relaxation as well as a growing library of digital  evidence-based mental health programs  Calm offers trusted support for individuals and organizations alike. Our flagship consumer app provides personalized content and activities – featuring a range of experts and beloved celebrity voices – to help users manage stress  improve sleep and live mindfully. Our workplace and healthcare solutions offer a consumer-friendly approach to clinical content and HIPAA-compliant resources in order to drive positive health and business outcomes. Named a TIME100 Most Influential Company  Calm supports more than 150 million people and 3 500 organizations across seven languages and 190 countries.What We DoAs a data organization  we focus on making data a competitive advantage for Calm. We’re product-minded  team-oriented  and grounded in our mission of making the world a happier and healthier place. We work closely with teams across the company such as product  finance  marketing  data science  and more. As a team  we strive to always improve.What You’ll DoWe’re looking for someone who is comfortable with ambiguity  assesses what needs to be done  and delivers with the right balance of velocity and technical debt. As a Senior Data Engineer  you’ll leverage all sorts of data  from application event streams to product databases to third-party data  to help stakeholders create products and answer business questions. Our stack spans AWS and GCP  with technologies like Airflow  Redshift  BigQuery  Postgres  Spark  and dbt. Specifically  you will:Work with business stakeholders to understand their goals  challenges  and decisionsIdentify opportunities and build solutions that standardize our data approach to common problems across the companyEvangelize the use of data-driven decision making across the organizationBuild processes to ensure our data is trusted and well-documentedPartner with data analysts on refining the data model used for reporting and analytical purposesCollaborate with engineering on improving availability and consistency of data points crucial for analysis and represent data team in architectural discussionsDevelop  mentor and train data engineersSome past projects include:Standing up a reporting system in BigQuery from scratch  including data replication  infrastructure setup  dbt model creation  and integration with reporting endpointsCreating a user-level feature store and related API endpoints to support machine learning tasks such as content recommendation and persona creationRemodeling a critical data pipeline to decrease our model count by 50% and reduce run time by 83%Setting up scalable APIs to integrate our Data Warehouse with 3rd party applications for personalization that reaches tens of millions of customersRevamping orchestration and execution to reduce critical data delivery times by 70%Who You AreProficiency with SQL and an object-oriented languageExperience with RDBMS  data warehouses  and event systemsExperience in building data pipelines that scaleKnowledge of different data modeling paradigms  e.g. relational  data vault  and medallionAbility to translate non-technical business requirements into technical solutions  and translate technical solutions to business outcomesStrong communication skillsPragmatism: balancing scrappiness and rigorNice to HavesPython programing experienceExperience with data lakesExperience building across cloudsSome experience in Infrastructure as Code tools like TerraformMinimum RequirementsThis role typically requires 8+ years of related experienceThe anticipated salary range for this position is $185 500- $259 700. The base salary range represents the low and high end of Calm’s salary range for this position. Not all candidates will be eligible for the upper end of the salary range. Exact salary will ultimately depend on multiple factors  which may include the successful candidate's geographic location  skills  experience and other qualifications. This role is also eligible for equity + comprehensive benefits + 401k + flexible time off.Please note that Calm may leverage artificial intelligence technology in the application review process.Calm is committed to providing reasonable accommodations for qualified individuals with disabilities  including disabled veterans. Please contact Calm’s Recruiting team if you need a reasonable accommodation  assistance completing any forms  or to otherwise participate in the application process. You can reach the Recruiting team at recruitingaccommodations@calm.comWe believe that mental health is health  and every person should be considered in the discussion. That’s why we’re proud to be an equal opportunity workplace  committed to providing equal employment opportunities to all applicants and employees regardless of race  color  religion  national origin  age  sex  marital status  ancestry  physical or mental disability  medical condition  genetic information  military or veteran status  gender identity or expression  sexual orientation  or any other characteristic protected by applicable federal  state or local law.Calm is deeply committed to diversity  equity and inclusion. We strive to create a mindful and respectful environment where everyone can bring their authentic self to work  and experience a culture that is free of harassment  racism  and discrimination.Employment offers are contingent upon the successful completion of a background check. Roles which require access to certain types of information may also require the successful completion of a drug screening.Calm participates in e-verify. E-verify provides the federal government with your Form I-9 information to confirm that you are authorized to work in the U.S.Right to WorkE-Verify Participation,Mid-Senior level,Full-time
Join our team as a Data Engineer  where you'll leverage your technical prowess and business acumen to drive performance improvements through data and analytics. Your role involves uncovering insights within extensive data sets  collaborating with stakeholders to optimize processes  and communicating data-driven solutions to enhance business outcomes.Responsibilities:Uncover actionable insights by navigating and analyzing unstructured data  integrating datasets  and utilizing SQL for data extraction.Collect business requirements  validate data  and develop dashboards and reports to support decision-making.Collaborate across technical and operational teams to deliver data solutions that meet business and governance requirements  utilizing operational  financial  and clinical data.Requirements:Bachelor's degree in healthcare  analytics  computer science  or related field; Master's degree preferred.3-4 years of experience in data collection  analysis  and interpretation  with proficiency in Tableau Desktop and database development.Strong analytical skills  proficiency in SQL  and experience with statistical analysis techniques. Excellent communication and interpersonal skills are essential.,Mid-Senior level,Full-time
Job DescriptioniTalent digital is seeking a Senior Data Engineer to join our dynamic team of technology consultants. This role is 100% remote working in PST timezone!Required qualifications and skills8-10 years of software development and deployment experience with hands-on experience with SQL  Datastage (or other ETL tool).Experience with Databricks  Azure  AWS or equivalent cloud platformsExperience in analyzing huge datasets  identify trends  patterns  and outliers to extract meaningful insightsStrong experience with data modeling  design patterns  building highly scalable Business Intelligence Solutions and distributed applicationsKnowledge of cloud platforms  for example:Experience with storing  joining  filtering  and analyzing data using SQL  Spark  Hive etc.Experience working with continuous integration framework  building regression-able code within data world using GitHub  Jenkins and related applicationsExperience with programming/scripting languages such as Scala/Java/Python/R etc. (any combination)Experience building data ingestion pipelines (simulating Extract  Transform  Load workload)  data warehouse or database architecturePreferred qualifications and skillsSkill set Big Data: Scala  Databricks  Spark SQL  Nice to Have: Spark Streaming  Python  Pyspark Cloud: Azure   ADF System Design: API Design  Event driven architecture. Microservice: Core Java 8 or above  SpringBoot  Azure Functions Front end ( Nice to Have ) EducationBachelors DegreeCompany descriptionAbout iTalent Digital:​ ​ We are part of a new generation of consulting and software development company that blends diversity  innovation  and integrity with real business results. Our structure rejects any strong hierarchy  empowering us to deliver excellent results. We are a woman- and minority-led firm. Every day  we challenge ourselves to be considerate  fair and to re-think what great outcomes mean for our customers. This permeates down to how we approach every interaction  on every project  for every client. You’ll thrive here if you are a dynamic self-starter  a difference-maker or someone who wants to deliver great results  without constraints. ​ The iTalent Digital Experience:​ ​ Joining us means you’ll be part of our global community  you have a say about your own career journey  and you’ll get a chance to give back to causes that matter. You will experience working with Fortune 500 companies and high-performance teams across numerous industries.  iTalent Digital offers our employees excellent benefits such as medical  dental  vision  life insurance  paid holidays  PTO  401K + matching  networking & career learning and development programs. We are growing and we want to see you grow!​ ​ Visit https://www.italentdigital.com/careers to learn more about what working at iTalent can mean for you.  All qualified applicants will receive consideration for employment without regard to race  color  religion  sex  sexual orientation  gender identity  national origin  age  disability or protected veteran status  or any other legally protected basis  in accordance with applicable law.  iTalent is committed to working with and providing reasonable accommodation to individuals with disabilities. If  because of a medical condition or disability  you need a reasonable accommodation for any part of the application process  or to perform the essential functions of a position  please contact us at recruiting@italentdigital.com and let us know the nature of your request and your contact information.,Mid-Senior level,Full-time
